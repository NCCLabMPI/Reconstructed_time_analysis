---
title: "ReconTime"
output: html_document
date: "2023-04-05"
---

#clear

```{r}
# clear workspace
rm(list = ls())
```

# Housekeeping

```{r}
wd = 'C:/Users/micha.engeser/Documents/GitHub/Reconstructed_time_analysis'
setwd(wd)
dir()
if (!('tidyverse' %in% installed.packages()))
{install.packages("tidyverse")}
library(tidyverse)

if (!('dplyr' %in% installed.packages()))
{install.packages('dplyr')}
library(dplyr)

if (!('brms' %in% installed.packages()))
{install.packages('brms')}
library(brms)

if (!('lme4' %in% installed.packages()))
{install.packages('lme4')}
library(lme4)

if (!('lmerTest' %in% installed.packages()))
{install.packages('lmerTest')}
library(lmerTest)

if (!('optimx' %in% installed.packages()))
{install.packages('optimx')}
library(optimx)

```

# details of subjects  

```{r}
subNums = c(101, 102, 103)
ses = 1
Lab_ID = 'SX'
task = 'prp'
```

# loading files

```{r}
count = 0
sub_ids = NaN
for (subNum in subNums ){
count = count + 1
sub_ids[count] = paste0(Lab_ID, subNum)

sub_folder = paste0('sub-', Lab_ID, subNum)
ses_folder = paste0('ses-', ses)
file_name = paste0(sub_folder, '_', ses_folder, '_run-all_task-', task,'_events.csv')

setwd("..")
parent_dir = getwd()
setwd(wd)
file = file.path(parent_dir, 'Reconstructed_time_experiment', 'pilot_data', sub_folder, ses_folder, file_name)
event_table = read.csv(file)

# make format consistent
if ('X' %in% colnames(event_table))
{event_table$X <- NULL}

colnames(event_table)[which(names(event_table) == "has_repsonse_vis")] <- "has_response_vis"
colnames(event_table)[which(names(event_table) == "trial_repsonse_vis")] <- "trial_response_vis"

# add subject number column 
event_table = event_table %>% mutate(sub_num = subNum)

# concatanate the tables
if ( exists('all_event_table')){
  all_event_table = rbind(all_event_table, event_table)
} else {
  all_event_table = event_table
}
}
```

# apply trial exclusion

```{r}

# remove target trials 
all_event_table = all_event_table[all_event_table$task_relevance != 'target', ]
relevant_event_table = all_event_table[all_event_table$task_relevance == 'non-target', ]
irrelevant_event_table = all_event_table[all_event_table$task_relevance == 'irrelevant', ]

# remove false alarm 
all_event_table = all_event_table[all_event_table$trial_response_vis != 'fa', ]

# remove RT < 100 ms
all_event_table = all_event_table[all_event_table$RT_aud > 0.1, ]

# remove incorrect auditory responses
all_event_table = all_event_table[all_event_table$trial_accuracy_aud == 1, ]

# remove outliers

# sample_mean = mean(all_event_table$RT_aud, na.rm = TRUE)
# sample_sd = sd(all_event_table$RT_aud, na.rm = TRUE)
# remove RT > 3x std
# all_event_table = all_event_table[all_event_table$RT_aud < sample_mean + 4*sample_sd, ]

# remove RT < 3x std
# all_event_table = all_event_table[all_event_table$RT_aud > sample_mean - 4*sample_sd, ]

# remove NaNs from auditory responses
all_event_table = all_event_table[!is.na(all_event_table$trial_accuracy_aud), ]


# sum(is.na(all_event_table$trial_accuracy_aud[all_event_table$SOA_lock == 'offset']), na.rm = TRUE)
# sum(is.na(all_event_table$trial_accuracy_aud[all_event_table$SOA_lock == 'onset']), na.rm = TRUE)

# check if any combination of condition (cell) has too little trials 

task_relevance_lvl = c('non-target', 'irrelevant')
SOA_lock_lvl = c('onset', 'offset')
SOA_lvl = c(0,0.116,0.232,0.466)
duration_lvl = c(0.5,1,1.5)
sub_id_lvl = sub_ids


for (o in sub_id_lvl){
  for (i in SOA_lvl){
    for (ii in SOA_lock_lvl) {
      for (iii in task_relevance_lvl) {
        for (iv in duration_lvl) {
          
          cell_trials = sum(all_event_table$sub_id == o & all_event_table$SOA == i & all_event_table$SOA_lock == ii & all_event_table$task_relevance == iii & all_event_table$duration == iv, na.rm = TRUE)
          

          if(cell_trials < 30){
            cell_name = paste(i,ii,iii,iv,sep= '-')
            warning(paste0('Subject ',o,' has only ', cell_trials,' trials for cell: ', cell_name, '\n'))
          }
        }}}}}

```

# process data 

```{r}

# take mean for each participant
task_relevance_lvl = c('non-target', 'irrelevant')
SOA_lock_lvl = c('onset', 'offset')
SOA_lvl = c(0,0.116,0.232,0.466)
duration_lvl = c(0.5,1,1.5)
sub_id_lvl = sub_ids


all_data = data.frame(matrix(ncol = 0, nrow = 1))

all_data$sub_id = "NaN"
all_data$SOA = NaN
all_data$SOA_lock = "NaN"
all_data$task_relevance = "NaN"
all_data$duration = NaN
all_data$mean_RT = NaN 
all_data$sd_RT = NaN

all_data = all_data[-c(1), ]

counter = 0
for (o in sub_id_lvl){
  for (i in SOA_lvl){
    for (ii in SOA_lock_lvl) {
      for (iii in task_relevance_lvl) {
        for (iv in duration_lvl) {
          
          counter = counter + 1 
          all_data = all_data %>% add_row(sub_id = o, SOA = i, SOA_lock = ii, task_relevance = iii, duration = iv,
                                          mean_RT = mean(all_event_table$RT_aud[all_event_table$sub_id == o & all_event_table$SOA == i & all_event_table$SOA_lock == ii & all_event_table$task_relevance == iii & all_event_table$duration == iv], na.rm = TRUE),
                                          sd_RT = sd(all_event_table$RT_aud[all_event_table$sub_id == o & all_event_table$SOA == i & all_event_table$SOA_lock == ii & all_event_table$task_relevance == iii & all_event_table$duration == iv], na.rm = TRUE))
}}}}}

all_data = all_data[1:(counter), ]

SOA_SOA_lock_data = all_data %>%  group_by(SOA, SOA_lock) %>%
                                      summarise(mean_RT = mean(mean_RT, na.rm = TRUE),
                                                         sd_RT = mean(sd_RT, na.rm = TRUE))

SOA_SOA_lock_task_relevance_data = all_data %>%  group_by(SOA, SOA_lock, task_relevance) %>%
                                      summarise(mean_RT = mean(mean_RT, na.rm = TRUE),
                                                         sd_RT = mean(sd_RT, na.rm = TRUE))

SOA_duration_data = all_data %>%  group_by(SOA, duration) %>%
                                      summarise(mean_RT = mean(mean_RT, na.rm = TRUE),
                                                         sd_RT = mean(sd_RT, na.rm = TRUE))


SOA_SOA_lock_duration_data = all_data %>%  group_by(SOA, SOA_lock, duration) %>%
                                      summarise(mean_RT = mean(mean_RT, na.rm = TRUE),
                                                         sd_RT = mean(sd_RT, na.rm = TRUE))
```

## TRANSFORM DATA

``` {r}
# add log transformed column
all_event_table = all_event_table %>% mutate(log_RT_aud = log(RT_aud))

# add response window column 
all_event_table = all_event_table %>% mutate(resp_window = (2-onset_SOA)+stim_jit )

# make task relevance and SOA lock numeric
all_event_table = all_event_table %>% mutate(is_task_relevant = ifelse(task_relevance == 'non-target',1,-1)) 
all_event_table = all_event_table %>% mutate(is_onset = ifelse(SOA_lock == 'onset',1,-1))
all_event_table = all_event_table %>% mutate(ori_num = unclass(as.factor(orientation))) # 1=center, 2=left, 3=right 
all_event_table = all_event_table %>% mutate(cate_num = unclass(as.factor(category))) # 1=face, 2=false_font, 3=letter, 4=object

# center variables 
all_event_table = all_event_table %>% mutate(c_task_relevant = is_task_relevant - mean(is_task_relevant))
all_event_table = all_event_table %>% mutate(c_is_onset = is_onset - mean(is_onset))
all_event_table = all_event_table %>% mutate(c_pitch = (pitch-1050)/50) # makes pitches 1 for high and -1 for low

```
# plotting

```{R}
# make duration a string
event_table2 = all_event_table
event_table2$duration = as.character(event_table2$duration)

# histograms

#all
hist_1 = ggplot(event_table2, aes(x=RT_aud)) + 
  geom_histogram(bins = 30) + xlim(c(0,2))
hist_1
# split by duration
hist_2 = ggplot(event_table2, aes(x=RT_aud, fill=duration)) +
  geom_histogram(alpha=0.6, position='identity') + xlim(c(0,2))
hist_2
# split by task relevance
hist_3 = ggplot(event_table2, aes(x=RT_aud, fill=task_relevance)) +
  geom_histogram(alpha=0.6, position='identity') + xlim(c(0,2))
hist_3
# split by SOA-lock
hist_4 = ggplot(event_table2, aes(x=RT_aud, fill=SOA_lock)) +
  geom_histogram(alpha=0.6, position='identity') + xlim(c(0,2))
hist_4


# line plots 

p1 = ggplot(data=SOA_SOA_lock_data, aes(x=SOA*1000, y=mean_RT*1000, group=SOA_lock)) +
  geom_line(aes(linetype = SOA_lock))+
  geom_point()+
  ylim(c(400,700))+
  ylab("Reaction time [ms]")+
  xlab('SOA [ms]')
p1

p2 = ggplot(data=SOA_SOA_lock_duration_data, aes(x=SOA*1000, y=mean_RT*1000, group=interaction(as.character(duration), SOA_lock))) +
  geom_line(aes(color=as.character(duration), linetype = SOA_lock))+
  geom_point()+
  ylim(c(400,700))+
  ylab("Reaction time [ms]")+
  xlab('SOA [ms]')+
  scale_color_manual(values=c('red','blue','green'))
p2

p3 = ggplot(data=SOA_SOA_lock_task_relevance_data, aes(x=SOA*1000, y=mean_RT*1000, group=interaction(task_relevance, SOA_lock))) +
  geom_line(aes(color=task_relevance, linetype = SOA_lock))+
  geom_point()+
  ylim(c(400,700))+
  ylab("Reaction time [ms]")+
  xlab('SOA [ms]')+
  scale_color_manual(values=c('red','blue'))
p3



```

```{r}
# box plots 
box_plot_data = data.frame(matrix(ncol = 0, nrow = 0))
counter2 = 0
col_names = "nan"

  for (i in SOA_lock_lvl){
    for (ii in SOA_lvl) {
      for (iii in task_relevance_lvl) {
        for (iv in duration_lvl) {
                    
          counter2 = counter2 + 1 
          col_names[counter2] = paste(i,ii,iii,iv,sep = "_")
          for (o in 1:length(subNums)){

          box_plot_data[o,counter2] = mean(all_event_table$RT_aud[all_event_table$sub_id == sub_id_lvl[o] & all_event_table$SOA_lock == i & all_event_table$SOA == ii & all_event_table$task_relevance == iii & all_event_table$duration == iv], na.rm = TRUE)
}}}}}


colors = c(rep("red",3),rep("darkred",3),rep("green",3),rep("darkgreen",3),rep("blue",3),rep("darkblue",3),rep("yellow",3), rep("orange",3))

ggplot(all_data, aes(SOA_lock, mean_RT, fill=interaction(duration, task_relevance, SOA))) +
  stat_boxplot(geom ='errorbar')+
  geom_boxplot() +
  scale_fill_manual(values=colors)

```

# main analysis

```{r}

# full general linear mixed effect model

full_model = glmer(RT_aud ~ c_SOA*is_onset*c_duration*is_task_relevant + (1*c_SOA*is_onset*c_duration*is_task_relevant|sub_id), family=Gamma(link="log"), data=all_event_table)
summary(full_model)
plot(fitted(full_model),residuals(full_model))
qqnorm(residuals(full_model))

# reduced general linear mixed effect model (without duration, if duration is not significant in the full model)

reduced_model = glmer(RT_aud ~ c_SOA*is_onset*c_duration*is_task_relevant + (1c_SOA*is_onset*c_duration*is_task_relevant|sub_id), family=Gamma(link="log"), data=all_event_table)
summary(reduced_model)
plot(fitted(reduced_model),residuals(reduced_model))
qqnorm(residuals(reduced_model))

```

# exploratory control analayses

```{r}

# control model

pitch_model = glmer(RT_aud ~ c_SOA*is_onset*c_duration*is_task_relevant*c_pitch + (1*c_SOA*is_onset*c_duration*is_task_relevant*c_pitch|sub_id), family=Gamma(link="log"), data=all_event_table)
summary(pitch_model)

cate_model = glmer(RT_aud ~ c_SOA*is_onset*c_duration*is_task_relevant*cate_num + (1*c_SOA*is_onset*c_duration*is_task_relevant*cate_num|sub_id), family=Gamma(link="log"), data=all_event_table)
summary(cate_model)

ori_model = glmer(RT_aud ~ c_SOA*is_onset*c_duration*is_task_relevant*ori_num + (1*c_SOA*is_onset*c_duration*is_task_relevant*ori_num|sub_id), family=Gamma(link="log"), data=all_event_table)
summary(ori_model)

```

# bayesian exploratory analysis

```{r}

library(brms)
install.packages("dampack")
library(dampack)



# data simulation and priors
gamma_params = gamma_params(mean(all_event_table$RT_aud, na.rm = TRUE), sd(event_table$RT_aud, na.rm = TRUE), scale = FALSE)

prior1 = rgamma(n=1000,shape=gamma_params$shape,rate=gamma_params$rate)

prior_v <-
c(
prior(normal(0, 10), class = Intercept),
prior(normal(0, 10), class = b, coef = SOA),
prior(normal(0, 50), class = sigma),
prior(normal(0, 20), class = sd, coef = Intercept, group = sub_id),
prior(normal(0, 20), class = sd, coef = SOA, group = sub_id)
)

#
brm_SOA_SOA_lock_model = brm(RT_aud ~ SOA*SOA_lock +(1|sub_id), all_event_table, family=Gamma(),save_all_pars = TRUE)
summary(brm_SOA_SOA_lock_model)
confint(brm_SOA_SOA_lock_model)
plot(brm_SOA_SOA_lock_model)

brm_null_model = brm(RT_aud ~ 1, all_event_table, family=Gamma(),save_all_pars = TRUE)
summary(brm_null_model)
confint(brm_null_model)
# Calculate the Bayes factor for the model with and without the parameter of interest
bayes_factor = bayes_factor(brm_SOA_SOA_lock_model, brm_null_model)


brm_main_mixed_model = brm(RT_aud ~ SOA*SOA_lock*duration*task_relevance + (1+SOA+SOA_lock+duration+task_relevance|sub_id), family = Gamma(), data=all_event_table)
summary(brm_main_mixed_model)




```

# power analysis

````{r}
if (!('pwr' %in% installed.packages()))
{install.packages("pwr")}
library(pwr)


# from marti paper
f =  47.08
n = 10
df_num = 5
df_denom = 45

# effect size 
eta = (f * df_num ) / (f * df_num  + df_denom )
effect_size = sqrt((df_denom/n)*(eta/(1-eta)))

# Specify  desired power, and significance level
power = 0.80
alpha = 0.05

# degrees of freedom for the numerator for our analysis 
u = 1

# Estimate sample size for GLMM with logistic regression
v = pwr.f2.test(u = u, v = , f2 = effect_size/4, sig.level = .05, power = power)$v

req_sample_size = ceiling(v + u + 1)
  
# (v = n - u - 1\). This implies \(n = v + u + 1\).


```
(Intercept)               0.516229   0.060345   8.555
SOA                      -0.149264   0.021116  -7.069
SOA_lockonset             0.053079   0.027122   1.957
duration                  0.004461   0.006218   0.717
task_relevancenon-target  0.031554   0.016313   1.934
